#pragma once

// @generated by aten/src/ATen/gen.py

#include <ATen/ScalarType.h>
#include <ATen/TensorOperators.h>
#include <ATen/TensorMethods.h>
#include <ATen/TensorOptions.h>

#include <array>
#include <functional>
#include <string>
#include <tuple>
#include <vector>

namespace at {
struct Generator;
class Scalar;
struct Tensor;
struct Type;
} // namespace at

namespace at {
namespace native {

inline Tensor from_blob(
    void* data,
    IntList sizes,
    const std::function<void(void*)>& deleter,
    const TensorOptions& options = {}) {
  return options.type().tensorFromBlob(data, sizes, deleter);
}

inline Tensor from_blob(
    void* data,
    IntList sizes,
    const TensorOptions& options = {}) {
  return native::from_blob(data, sizes, /*deleter=*/[](void*) {}, options);
}

// These functions are defined in native/TensorFactories.cpp.
#define TENSOR(T, S, _1)                                               \
  AT_API Tensor tensor(ArrayRef<T> values, const TensorOptions& options);     \
  inline Tensor tensor(                                                \
      std::initializer_list<T> values, const TensorOptions& options) { \
    return native::tensor(ArrayRef<T>(values), options);               \
  }                                                                    \
  inline Tensor tensor(T value, const TensorOptions& options) {        \
    return native::tensor(ArrayRef<T>(value), options);                \
  }                                                                    \
  inline Tensor tensor(ArrayRef<T> values) {                           \
    return native::tensor(std::move(values), at::dtype(k##S));         \
  }                                                                    \
  inline Tensor tensor(std::initializer_list<T> values) {              \
    return native::tensor(ArrayRef<T>(values));                        \
  }                                                                    \
  inline Tensor tensor(T value) {                                      \
    return native::tensor(ArrayRef<T>(value));                         \
  }
AT_FORALL_SCALAR_TYPES_EXCEPT_HALF(TENSOR)
#undef TENSOR

AT_API Tensor _cast_Byte(const Tensor & self, bool non_blocking=false);
AT_API Tensor _cast_Char(const Tensor & self, bool non_blocking=false);
AT_API Tensor _cast_Double(const Tensor & self, bool non_blocking=false);
AT_API Tensor _cast_Float(const Tensor & self, bool non_blocking=false);
AT_API Tensor _cast_Int(const Tensor & self, bool non_blocking=false);
AT_API Tensor _cast_Long(const Tensor & self, bool non_blocking=false);
AT_API Tensor _cast_Short(const Tensor & self, bool non_blocking=false);
AT_API Tensor _cast_Half(const Tensor & self, bool non_blocking=false);
AT_API std::tuple<Tensor,Tensor> _cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntList input_lengths, IntList target_lengths, int64_t blank, bool deterministic);
AT_API Tensor _cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional);
AT_API std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntList batch_sizes, const Tensor & dropout_state);
AT_API std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> _cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntList batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask);
AT_API Tensor _cudnn_init_dropout_state(const Type & self_ty, double dropout, bool train, int64_t dropout_seed);
AT_API std::tuple<Tensor,Tensor> fused_dropout_cuda(const Tensor & self, double p, Generator * generator=nullptr);
AT_API Tensor masked_scale_cuda(const Tensor & self, const Tensor & mask, double scale);
AT_API Tensor dropout(const Tensor & input, double p, bool train);
AT_API Tensor & dropout_(Tensor & self, double p, bool train);
AT_API Tensor feature_dropout(const Tensor & input, double p, bool train);
AT_API Tensor & feature_dropout_(Tensor & self, double p, bool train);
AT_API Tensor alpha_dropout(const Tensor & input, double p, bool train);
AT_API Tensor & alpha_dropout_(Tensor & self, double p, bool train);
AT_API Tensor feature_alpha_dropout(const Tensor & input, double p, bool train);
AT_API Tensor & feature_alpha_dropout_(Tensor & self, double p, bool train);
AT_API Tensor abs(const Tensor & self);
AT_API Tensor & _abs__cpu(Tensor & self);
AT_API Tensor & _abs__cuda(Tensor & self);
AT_API Tensor & _abs_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _abs_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor acos(const Tensor & self);
AT_API Tensor & _acos__cpu(Tensor & self);
AT_API Tensor & _acos__cuda(Tensor & self);
AT_API Tensor & _acos_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _acos_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor avg_pool1d(const Tensor & self, IntList kernel_size, IntList stride={}, IntList padding=0, bool ceil_mode=false, bool count_include_pad=true);
AT_API Tensor adaptive_avg_pool1d(const Tensor & self, IntList output_size);
AT_API std::tuple<Tensor,Tensor> adaptive_max_pool1d(const Tensor & self, IntList output_size);
AT_API Tensor add(const Tensor & self, const Tensor & other, Scalar alpha=1);
AT_API Tensor & add_(Tensor & self, const Tensor & other, Scalar alpha=1);
AT_API Tensor & add_out(Tensor & result, const Tensor & self, const Tensor & other, Scalar alpha=1);
AT_API Tensor add(const Tensor & self, Scalar other, Scalar alpha=1);
AT_API Tensor & add_(Tensor & self, Scalar other, Scalar alpha=1);
AT_API Tensor addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & addmv_(Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta=1, Scalar alpha=1);
AT_API Tensor addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & addr_(Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor all(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor & all_out(Tensor & result, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API bool allclose(const Tensor & self, const Tensor & other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false);
AT_API Tensor any(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor & any_out(Tensor & result, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor arange(Scalar start, Scalar end, const TensorOptions & options={});
AT_API Tensor arange(Scalar start, Scalar end, Scalar step, const TensorOptions & options={});
AT_API Tensor & arange_out(Tensor & result, Scalar start, Scalar end);
AT_API Tensor & arange_out(Tensor & result, Scalar start, Scalar end, Scalar step);
AT_API Tensor arange(Scalar end, const TensorOptions & options={});
AT_API Tensor & arange_out(Tensor & result, Scalar end);
AT_API Tensor _dim_arange(const Tensor & like, int64_t dim);
AT_API Tensor argmax(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor argmax(const Tensor & self);
AT_API Tensor _argmax(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor argmin(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor argmin(const Tensor & self);
AT_API Tensor _argmin(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor as_strided(const Tensor & self, IntList size, IntList stride);
AT_API Tensor & as_strided_(Tensor & self, IntList size, IntList stride);
AT_API Tensor as_strided(const Tensor & self, IntList size, IntList stride, int64_t storage_offset);
AT_API Tensor & as_strided_(Tensor & self, IntList size, IntList stride, int64_t storage_offset);
AT_API Tensor asin(const Tensor & self);
AT_API Tensor & _asin__cpu(Tensor & self);
AT_API Tensor & _asin__cuda(Tensor & self);
AT_API Tensor & _asin_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _asin_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor atan(const Tensor & self);
AT_API Tensor & _atan__cpu(Tensor & self);
AT_API Tensor & _atan__cuda(Tensor & self);
AT_API Tensor & _atan_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _atan_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor bartlett_window(int64_t window_length, const TensorOptions & options={});
AT_API Tensor bartlett_window(int64_t window_length, bool periodic, const TensorOptions & options={});
AT_API Tensor batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps, bool cudnn_enabled);
AT_API Tensor bernoulli(const Tensor & self, const Tensor & p, Generator * generator=nullptr);
AT_API Tensor bernoulli(const Tensor & self, double p, Generator * generator=nullptr);
AT_API Tensor bernoulli(const Tensor & self);
AT_API Tensor & bernoulli_(Tensor & self, const Tensor & p, Generator * generator=nullptr);
AT_API Tensor & bernoulli_(Tensor & self, double p, Generator * generator=nullptr);
AT_API Tensor & bernoulli_(Tensor & self);
AT_API Tensor bilinear(const Tensor & input1, const Tensor & input2, const Tensor & weight, const Tensor & bias);
AT_API Tensor _bincount_cpu(const Tensor & self, const Tensor & weights={}, int64_t minlength=0);
AT_API Tensor _bincount_cuda(const Tensor & self, const Tensor & weights={}, int64_t minlength=0);
AT_API Tensor blackman_window(int64_t window_length, const TensorOptions & options={});
AT_API Tensor blackman_window(int64_t window_length, bool periodic, const TensorOptions & options={});
AT_API std::vector<Tensor> broadcast_tensors(TensorList tensors);
AT_API Tensor cat(TensorList tensors, int64_t dim=0);
AT_API Tensor & cat_out(Tensor & result, TensorList tensors, int64_t dim=0);
AT_API Tensor ceil(const Tensor & self);
AT_API Tensor & _ceil__cpu(Tensor & self);
AT_API Tensor & _ceil__cuda(Tensor & self);
AT_API Tensor & _ceil_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _ceil_out_cuda(Tensor & result, const Tensor & self);
AT_API std::vector<Tensor> chunk(const Tensor & self, int64_t chunks, int64_t dim=0);
AT_API Tensor clamp(const Tensor & self, Scalar min, Scalar max);
AT_API Tensor & _clamp__cpu(Tensor & self, Scalar min, Scalar max);
AT_API Tensor & _clamp__cuda(Tensor & self, Scalar min, Scalar max);
AT_API Tensor & _clamp_out_cpu(Tensor & result, const Tensor & self, Scalar min, Scalar max);
AT_API Tensor & _clamp_out_cuda(Tensor & result, const Tensor & self, Scalar min, Scalar max);
AT_API Tensor clamp_max(const Tensor & self, Scalar max);
AT_API Tensor & _clamp_max__cpu(Tensor & self, Scalar max);
AT_API Tensor & _clamp_max__cuda(Tensor & self, Scalar max);
AT_API Tensor & _clamp_max_out_cpu(Tensor & result, const Tensor & self, Scalar max);
AT_API Tensor & _clamp_max_out_cuda(Tensor & result, const Tensor & self, Scalar max);
AT_API Tensor clamp_min(const Tensor & self, Scalar min);
AT_API Tensor & _clamp_min__cpu(Tensor & self, Scalar min);
AT_API Tensor & _clamp_min__cuda(Tensor & self, Scalar min);
AT_API Tensor & _clamp_min_out_cpu(Tensor & result, const Tensor & self, Scalar min);
AT_API Tensor & _clamp_min_out_cuda(Tensor & result, const Tensor & self, Scalar min);
AT_API bool cudnn_is_acceptable(const Tensor & self);
AT_API Tensor convolution(const Tensor & input, const Tensor & weight, const Tensor & bias, IntList stride, IntList padding, IntList dilation, bool transposed, IntList output_padding, int64_t groups);
AT_API Tensor _convolution(const Tensor & input, const Tensor & weight, const Tensor & bias, IntList stride, IntList padding, IntList dilation, bool transposed, IntList output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled);
AT_API Tensor _convolution_nogroup(const Tensor & input, const Tensor & weight, const Tensor & bias, IntList stride, IntList padding, IntList dilation, bool transposed, IntList output_padding);
AT_API std::tuple<Tensor,Tensor,Tensor> _convolution_double_backward(const Tensor & ggI, const Tensor & ggW, const Tensor & ggb, const Tensor & gO, const Tensor & weight, const Tensor & self, IntList stride, IntList padding, IntList dilation, bool transposed, IntList output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, std::array<bool,3> output_mask);
AT_API Tensor conv1d(const Tensor & input, const Tensor & weight, const Tensor & bias={}, IntList stride=1, IntList padding=0, IntList dilation=1, int64_t groups=1);
AT_API Tensor conv2d(const Tensor & input, const Tensor & weight, const Tensor & bias={}, IntList stride=1, IntList padding=0, IntList dilation=1, int64_t groups=1);
AT_API Tensor conv3d(const Tensor & input, const Tensor & weight, const Tensor & bias={}, IntList stride=1, IntList padding=0, IntList dilation=1, int64_t groups=1);
AT_API Tensor conv_tbc(const Tensor & self, const Tensor & weight, const Tensor & bias, int64_t pad);
AT_API std::tuple<Tensor,Tensor,Tensor> conv_tbc_backward(const Tensor & self, const Tensor & input, const Tensor & weight, const Tensor & bias, int64_t pad);
AT_API Tensor conv_transpose1d(const Tensor & input, const Tensor & weight, const Tensor & bias={}, IntList stride=1, IntList padding=0, IntList output_padding=0, int64_t groups=1, IntList dilation=1);
AT_API Tensor conv_transpose2d(const Tensor & input, const Tensor & weight, const Tensor & bias={}, IntList stride=1, IntList padding=0, IntList output_padding=0, int64_t groups=1, IntList dilation=1);
AT_API Tensor conv_transpose3d(const Tensor & input, const Tensor & weight, const Tensor & bias={}, IntList stride=1, IntList padding=0, IntList output_padding=0, int64_t groups=1, IntList dilation=1);
AT_API Tensor cos(const Tensor & self);
AT_API Tensor & _cos__cpu(Tensor & self);
AT_API Tensor & _cos__cuda(Tensor & self);
AT_API Tensor & _cos_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _cos_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor cosh(const Tensor & self);
AT_API Tensor & _cosh__cpu(Tensor & self);
AT_API Tensor & _cosh__cuda(Tensor & self);
AT_API Tensor & _cosh_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _cosh_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor cosine_embedding_loss(const Tensor & input1, const Tensor & input2, const Tensor & target, double margin=0.0, int64_t reduction=Reduction::ElementwiseMean);
AT_API Tensor cudnn_affine_grid_generator_forward(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W);
AT_API Tensor cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W);
AT_API std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon);
AT_API std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon);
AT_API Tensor cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API Tensor cudnn_convolution_backward_input(IntList self_size, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask);
AT_API Tensor cudnn_convolution_backward_bias(const Tensor & grad_output);
AT_API Tensor cudnn_convolution_backward_weight(IntList weight_size, const Tensor & grad_output, const Tensor & self, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API Tensor cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask);
AT_API Tensor cudnn_convolution_backward_bias(const Tensor & grad_output);
AT_API Tensor cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API Tensor cudnn_convolution_transpose_backward_weight(IntList weight_size, const Tensor & grad_output, const Tensor & self, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API Tensor cudnn_grid_sampler_forward(const Tensor & self, const Tensor & grid);
AT_API std::tuple<Tensor,Tensor> cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output);
AT_API Tensor cumsum(const Tensor & self, int64_t dim, ScalarType dtype);
AT_API Tensor cumsum(const Tensor & self, int64_t dim);
AT_API Tensor & cumsum_out(Tensor & result, const Tensor & self, int64_t dim, ScalarType dtype);
AT_API Tensor & cumsum_out(Tensor & result, const Tensor & self, int64_t dim);
AT_API Tensor cumprod(const Tensor & self, int64_t dim, ScalarType dtype);
AT_API Tensor cumprod(const Tensor & self, int64_t dim);
AT_API Tensor & cumprod_out(Tensor & result, const Tensor & self, int64_t dim, ScalarType dtype);
AT_API Tensor & cumprod_out(Tensor & result, const Tensor & self, int64_t dim);
AT_API Tensor ctc_loss(const Tensor & log_probs, const Tensor & targets, IntList input_lengths, IntList target_lengths, int64_t blank=0, int64_t reduction=Reduction::ElementwiseMean);
AT_API Tensor ctc_loss(const Tensor & log_probs, const Tensor & targets, const Tensor & input_lengths, const Tensor & target_lengths, int64_t blank=0, int64_t reduction=Reduction::ElementwiseMean);
AT_API std::tuple<Tensor,Tensor> ctc_loss_cpu(const Tensor & log_probs, const Tensor & targets, IntList input_lengths, IntList target_lengths, int64_t blank=0);
AT_API std::tuple<Tensor,Tensor> ctc_loss_gpu(const Tensor & log_probs, const Tensor & targets, IntList input_lengths, IntList target_lengths, int64_t blank=0);
AT_API Tensor ctc_loss_backward_cpu(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntList input_lengths, IntList target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank);
AT_API Tensor ctc_loss_backward_gpu(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntList input_lengths, IntList target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank);
AT_API Tensor det(const Tensor & self);
AT_API Tensor diagflat(const Tensor & self, int64_t offset=0);
AT_API Tensor diagonal(const Tensor & self, int64_t offset=0, int64_t dim1=0, int64_t dim2=1);
AT_API Tensor div(const Tensor & self, const Tensor & other);
AT_API Tensor & div_(Tensor & self, const Tensor & other);
AT_API Tensor & div_out(Tensor & result, const Tensor & self, const Tensor & other);
AT_API Tensor div(const Tensor & self, Scalar other);
AT_API Tensor & div_(Tensor & self, Scalar other);
AT_API Tensor dot(const Tensor & self, const Tensor & tensor);
AT_API Tensor & dot_out(Tensor & result, const Tensor & self, const Tensor & tensor);
AT_API Tensor einsum(std::string equation, TensorList tensors);
AT_API Tensor embedding(const Tensor & weight, const Tensor & indices, int64_t padding_idx=-1, bool scale_grad_by_freq=false, bool sparse=false);
AT_API Tensor embedding_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq, bool sparse);
AT_API Tensor embedding_dense_backward_cpu(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq);
AT_API Tensor embedding_dense_backward_cuda(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq);
AT_API Tensor & embedding_renorm_cpu_(Tensor & self, const Tensor & indices, double max_norm, double norm_type);
AT_API Tensor & embedding_renorm_cuda_(Tensor & self, const Tensor & indices, double max_norm, double norm_type);
AT_API Tensor embedding_sparse_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq);
AT_API std::tuple<Tensor,Tensor,Tensor,Tensor> embedding_bag(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq=false, int64_t mode=0, bool sparse=false);
AT_API std::tuple<Tensor,Tensor,Tensor,Tensor> _embedding_bag_cpu(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq=false, int64_t mode=0, bool sparse=false);
AT_API std::tuple<Tensor,Tensor,Tensor,Tensor> _embedding_bag_cuda(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq=false, int64_t mode=0, bool sparse=false);
AT_API Tensor _embedding_bag_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse);
AT_API Tensor _embedding_bag_sparse_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, int64_t num_weights, bool scale_grad_by_freq, int64_t mode);
AT_API Tensor _embedding_bag_dense_backward_cpu(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode);
AT_API Tensor _embedding_bag_dense_backward_cuda(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode);
AT_API Tensor empty(IntList size, const TensorOptions & options={});
AT_API Tensor & empty_out(Tensor & result, IntList size);
AT_API Tensor empty_like(const Tensor & self);
AT_API Tensor empty_like(const Tensor & self, const TensorOptions & options);
AT_API Tensor erf(const Tensor & self);
AT_API Tensor & _erf__cpu(Tensor & self);
AT_API Tensor & _erf__cuda(Tensor & self);
AT_API Tensor & _erf_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _erf_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor erfc(const Tensor & self);
AT_API Tensor & _erfc__cpu(Tensor & self);
AT_API Tensor & _erfc__cuda(Tensor & self);
AT_API Tensor & _erfc_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _erfc_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor exp(const Tensor & self);
AT_API Tensor & _exp__cpu(Tensor & self);
AT_API Tensor & _exp__cuda(Tensor & self);
AT_API Tensor & _exp_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _exp_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor expm1(const Tensor & self);
AT_API Tensor & _expm1__cpu(Tensor & self);
AT_API Tensor & _expm1__cuda(Tensor & self);
AT_API Tensor & _expm1_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _expm1_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor expand(const Tensor & self, IntList size, bool implicit=false);
AT_API Tensor expand_as(const Tensor & self, const Tensor & other);
AT_API Tensor eye(int64_t n, const TensorOptions & options={});
AT_API Tensor eye(int64_t n, int64_t m, const TensorOptions & options={});
AT_API Tensor & eye_out_cpu(Tensor & result, int64_t n);
AT_API Tensor & eye_out_cuda(Tensor & result, int64_t n);
AT_API Tensor & eye_out_cpu(Tensor & result, int64_t n, int64_t m);
AT_API Tensor & eye_out_cuda(Tensor & result, int64_t n, int64_t m);
AT_API Tensor flatten(const Tensor & self, int64_t start_dim=0, int64_t end_dim=-1);
AT_API Tensor & fill_(Tensor & self, Scalar value);
AT_API Tensor & fill_(Tensor & self, const Tensor & value);
AT_API Tensor floor(const Tensor & self);
AT_API Tensor & _floor__cpu(Tensor & self);
AT_API Tensor & _floor__cuda(Tensor & self);
AT_API Tensor & _floor_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _floor_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor full(IntList size, Scalar fill_value, const TensorOptions & options={});
AT_API Tensor & full_out(Tensor & result, IntList size, Scalar fill_value);
AT_API Tensor full_like(const Tensor & self, Scalar fill_value);
AT_API Tensor full_like(const Tensor & self, Scalar fill_value, const TensorOptions & options);
AT_API Tensor grid_sampler(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode);
AT_API Tensor grid_sampler_2d_cpu(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode);
AT_API Tensor grid_sampler_2d_cuda(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode);
AT_API std::tuple<Tensor,Tensor> grid_sampler_2d_backward_cpu(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode);
AT_API std::tuple<Tensor,Tensor> grid_sampler_2d_backward_cuda(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode);
AT_API Tensor grid_sampler_3d_cpu(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode);
AT_API Tensor grid_sampler_3d_cuda(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode);
AT_API std::tuple<Tensor,Tensor> grid_sampler_3d_backward_cpu(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode);
AT_API std::tuple<Tensor,Tensor> grid_sampler_3d_backward_cuda(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode);
AT_API Tensor hann_window(int64_t window_length, const TensorOptions & options={});
AT_API Tensor hann_window(int64_t window_length, bool periodic, const TensorOptions & options={});
AT_API Tensor hamming_window(int64_t window_length, const TensorOptions & options={});
AT_API Tensor hamming_window(int64_t window_length, bool periodic, const TensorOptions & options={});
AT_API Tensor hamming_window(int64_t window_length, bool periodic, double alpha, const TensorOptions & options={});
AT_API Tensor hamming_window(int64_t window_length, bool periodic, double alpha, double beta, const TensorOptions & options={});
AT_API Tensor hinge_embedding_loss(const Tensor & self, const Tensor & target, double margin=1.0, int64_t reduction=Reduction::ElementwiseMean);
AT_API Tensor ger(const Tensor & self, const Tensor & vec2);
AT_API Tensor & ger_out(Tensor & result, const Tensor & self, const Tensor & vec2);
AT_API std::tuple<Tensor,Tensor> gesv(const Tensor & self, const Tensor & A);
AT_API std::tuple<Tensor &,Tensor &> gesv_out(Tensor & solution, Tensor & lu, const Tensor & self, const Tensor & A);
AT_API std::tuple<Tensor,Tensor> _gesv_helper_cpu(const Tensor & self, const Tensor & A);
AT_API std::tuple<Tensor,Tensor> _gesv_helper_cuda(const Tensor & self, const Tensor & A);
AT_API Tensor group_norm(const Tensor & input, int64_t num_groups, const Tensor & weight={}, const Tensor & bias={}, double eps=1e-05, bool cudnn_enabled=true);
AT_API Tensor fft(const Tensor & self, int64_t signal_ndim, bool normalized=false);
AT_API Tensor ifft(const Tensor & self, int64_t signal_ndim, bool normalized=false);
AT_API Tensor rfft(const Tensor & self, int64_t signal_ndim, bool normalized=false, bool onesided=true);
AT_API Tensor irfft(const Tensor & self, int64_t signal_ndim, bool normalized=false, bool onesided=true, IntList signal_sizes={});
AT_API Tensor _fft_mkl(const Tensor & self, int64_t signal_ndim, bool complex_input, bool complex_output, bool inverse, IntList checked_signal_sizes, bool normalized, bool onesided, IntList output_sizes);
AT_API Tensor _fft_cufft(const Tensor & self, int64_t signal_ndim, bool complex_input, bool complex_output, bool inverse, IntList checked_signal_sizes, bool normalized, bool onesided, IntList output_sizes);
AT_API int64_t _cufft_get_plan_cache_size();
AT_API int64_t _cufft_get_plan_cache_max_size();
AT_API void _cufft_set_plan_cache_max_size(int64_t max_size);
AT_API void _cufft_clear_plan_cache();
AT_API Tensor index(const Tensor & self, TensorList indices);
AT_API Tensor & index_copy_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source);
AT_API Tensor index_put(const Tensor & self, TensorList indices, const Tensor & values);
AT_API Tensor & index_put_(Tensor & self, TensorList indices, const Tensor & values);
AT_API Tensor inverse(const Tensor & self);
AT_API Tensor & inverse_out(Tensor & result, const Tensor & self);
AT_API Tensor isclose(const Tensor & self, const Tensor & other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false);
AT_API bool is_cuda(const Tensor & self);
AT_API bool is_distributed(const Tensor & self);
AT_API bool is_floating_point(const Tensor & self);
AT_API bool is_nonzero(const Tensor & self);
AT_API bool is_same_size(const Tensor & self, const Tensor & other);
AT_API bool is_signed(const Tensor & self);
AT_API bool is_sparse(const Tensor & self);
AT_API std::tuple<Tensor,Tensor> kthvalue(const Tensor & self, int64_t k, int64_t dim=-1, bool keepdim=false);
AT_API std::tuple<Tensor &,Tensor &> kthvalue_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim=-1, bool keepdim=false);
AT_API Tensor layer_norm(const Tensor & input, IntList normalized_shape, const Tensor & weight={}, const Tensor & bias={}, double eps=1e-05, bool cudnn_enable=true);
AT_API Tensor linear(const Tensor & input, const Tensor & weight, const Tensor & bias={});
AT_API Tensor linspace(Scalar start, Scalar end, const TensorOptions & options={});
AT_API Tensor linspace(Scalar start, Scalar end, int64_t steps, const TensorOptions & options={});
AT_API Tensor & linspace_out(Tensor & result, Scalar start, Scalar end);
AT_API Tensor & linspace_out(Tensor & result, Scalar start, Scalar end, int64_t steps);
AT_API Tensor log(const Tensor & self);
AT_API Tensor & _log__cpu(Tensor & self);
AT_API Tensor & _log__cuda(Tensor & self);
AT_API Tensor & _log_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _log_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor log10(const Tensor & self);
AT_API Tensor & _log10__cpu(Tensor & self);
AT_API Tensor & _log10__cuda(Tensor & self);
AT_API Tensor & _log10_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _log10_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor log1p(const Tensor & self);
AT_API Tensor & _log1p__cpu(Tensor & self);
AT_API Tensor & _log1p__cuda(Tensor & self);
AT_API Tensor & log1p_sparse_(Tensor & self);
AT_API Tensor & _log1p_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _log1p_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor & log1p_out_sparse(Tensor & result, const Tensor & self);
AT_API Tensor log2(const Tensor & self);
AT_API Tensor & _log2__cpu(Tensor & self);
AT_API Tensor & _log2__cuda(Tensor & self);
AT_API Tensor & _log2_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _log2_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor logdet(const Tensor & self);
AT_API Tensor logspace(Scalar start, Scalar end, const TensorOptions & options={});
AT_API Tensor logspace(Scalar start, Scalar end, int64_t steps, const TensorOptions & options={});
AT_API Tensor & logspace_out(Tensor & result, Scalar start, Scalar end);
AT_API Tensor & logspace_out(Tensor & result, Scalar start, Scalar end, int64_t steps);
AT_API Tensor log_softmax_cpu(const Tensor & self, int64_t dim);
AT_API Tensor log_softmax_cuda(const Tensor & self, int64_t dim);
AT_API Tensor log_softmax_backward_cpu(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self);
AT_API Tensor log_softmax_backward_cuda(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self);
AT_API Tensor logsumexp(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor & logsumexp_out(Tensor & result, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor margin_ranking_loss(const Tensor & input1, const Tensor & input2, const Tensor & target, double margin=0.0, int64_t reduction=Reduction::ElementwiseMean);
AT_API Tensor matmul(const Tensor & self, const Tensor & other);
AT_API Tensor & matmul_out(Tensor & result, const Tensor & self, const Tensor & other);
AT_API Tensor matrix_rank(const Tensor & self, double tol, bool symmetric=false);
AT_API Tensor matrix_rank(const Tensor & self, bool symmetric=false);
AT_API std::tuple<Tensor,Tensor> max(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API std::tuple<Tensor &,Tensor &> max_out(Tensor & max, Tensor & max_values, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor max_values(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API std::tuple<Tensor,Tensor> max_pool1d_with_indices(const Tensor & self, IntList kernel_size, IntList stride={}, IntList padding=0, IntList dilation=1, bool ceil_mode=false);
AT_API Tensor max_pool1d(const Tensor & self, IntList kernel_size, IntList stride={}, IntList padding=0, IntList dilation=1, bool ceil_mode=false);
AT_API Tensor max_pool2d(const Tensor & self, IntList kernel_size, IntList stride={}, IntList padding=0, IntList dilation=1, bool ceil_mode=false);
AT_API Tensor max_pool3d(const Tensor & self, IntList kernel_size, IntList stride={}, IntList padding=0, IntList dilation=1, bool ceil_mode=false);
AT_API Tensor mean(const Tensor & self, ScalarType dtype);
AT_API Tensor mean(const Tensor & self);
AT_API Tensor mean(const Tensor & self, int64_t dim, bool keepdim, ScalarType dtype);
AT_API Tensor mean(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor mean(const Tensor & self, int64_t dim, ScalarType dtype);
AT_API Tensor & mean_out(Tensor & result, const Tensor & self, int64_t dim, bool keepdim, ScalarType dtype);
AT_API Tensor & mean_out(Tensor & result, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor & mean_out(Tensor & result, const Tensor & self, int64_t dim, ScalarType dtype);
AT_API std::tuple<Tensor,Tensor> median(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API std::tuple<Tensor &,Tensor &> median_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API std::tuple<Tensor,Tensor> min(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API std::tuple<Tensor &,Tensor &> min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor min_values(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor mkldnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntList padding, IntList stride, IntList dilation, int64_t groups);
AT_API Tensor mkldnn_convolution_backward_input(IntList self_size, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool bias_defined);
AT_API std::tuple<Tensor,Tensor> mkldnn_convolution_backward_weights(IntList weight_size, const Tensor & grad_output, const Tensor & self, IntList padding, IntList stride, IntList dilation, int64_t groups, bool bias_defined);
AT_API std::tuple<Tensor,Tensor,Tensor> mkldnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, std::array<bool,3> output_mask);
AT_API std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon);
AT_API std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon);
AT_API Tensor miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API Tensor miopen_convolution_backward_input(IntList self_size, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API std::tuple<Tensor,Tensor,Tensor> miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask);
AT_API Tensor miopen_convolution_backward_bias(const Tensor & grad_output);
AT_API Tensor miopen_convolution_backward_weight(IntList weight_size, const Tensor & grad_output, const Tensor & self, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API Tensor miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API std::tuple<Tensor,Tensor,Tensor> miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask);
AT_API Tensor miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API Tensor miopen_convolution_transpose_backward_weight(IntList weight_size, const Tensor & grad_output, const Tensor & self, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic);
AT_API Tensor mm(const Tensor & self, const Tensor & mat2);
AT_API Tensor & mm_out(Tensor & result, const Tensor & self, const Tensor & mat2);
AT_API std::tuple<Tensor,Tensor> mode(const Tensor & self, int64_t dim=-1, bool keepdim=false);
AT_API std::tuple<Tensor &,Tensor &> mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim=-1, bool keepdim=false);
AT_API Tensor mul(const Tensor & self, const Tensor & other);
AT_API Tensor & mul_(Tensor & self, const Tensor & other);
AT_API Tensor & mul_out(Tensor & result, const Tensor & self, const Tensor & other);
AT_API Tensor mul(const Tensor & self, Scalar other);
AT_API Tensor & mul_(Tensor & self, Scalar other);
AT_API Tensor mv(const Tensor & self, const Tensor & vec);
AT_API Tensor & mv_out(Tensor & result, const Tensor & self, const Tensor & vec);
AT_API Tensor mvlgamma(const Tensor & self, int64_t p);
AT_API Tensor & mvlgamma_(Tensor & self, int64_t p);
AT_API Tensor narrow(const Tensor & self, int64_t dim, int64_t start, int64_t length);
AT_API Tensor ones(IntList size, const TensorOptions & options={});
AT_API Tensor & ones_out(Tensor & result, IntList size);
AT_API Tensor ones_like(const Tensor & self);
AT_API Tensor ones_like(const Tensor & self, const TensorOptions & options);
AT_API Tensor pairwise_distance(const Tensor & x1, const Tensor & x2, double p=2, double eps=1e-06, bool keepdim=false);
AT_API Tensor permute(const Tensor & self, IntList dims);
AT_API Tensor pin_memory(const Tensor & self);
AT_API Tensor pinverse(const Tensor & self, double rcond=1e-15);
AT_API Tensor rand(IntList size, const TensorOptions & options={});
AT_API Tensor rand(IntList size, Generator * generator, const TensorOptions & options={});
AT_API Tensor & rand_out(Tensor & result, IntList size);
AT_API Tensor & rand_out(Tensor & result, IntList size, Generator * generator);
AT_API Tensor rand_like(const Tensor & self);
AT_API Tensor rand_like(const Tensor & self, const TensorOptions & options);
AT_API Tensor randint(int64_t high, IntList size, const TensorOptions & options={});
AT_API Tensor randint(int64_t high, IntList size, Generator * generator, const TensorOptions & options={});
AT_API Tensor randint(int64_t low, int64_t high, IntList size, const TensorOptions & options={});
AT_API Tensor randint(int64_t low, int64_t high, IntList size, Generator * generator, const TensorOptions & options={});
AT_API Tensor & randint_out(Tensor & result, int64_t high, IntList size);
AT_API Tensor & randint_out(Tensor & result, int64_t high, IntList size, Generator * generator);
AT_API Tensor & randint_out(Tensor & result, int64_t low, int64_t high, IntList size);
AT_API Tensor & randint_out(Tensor & result, int64_t low, int64_t high, IntList size, Generator * generator);
AT_API Tensor randint_like(const Tensor & self, int64_t high);
AT_API Tensor randint_like(const Tensor & self, int64_t low, int64_t high);
AT_API Tensor randint_like(const Tensor & self, int64_t high, const TensorOptions & options);
AT_API Tensor randint_like(const Tensor & self, int64_t low, int64_t high, const TensorOptions & options);
AT_API Tensor randn(IntList size, const TensorOptions & options={});
AT_API Tensor randn(IntList size, Generator * generator, const TensorOptions & options={});
AT_API Tensor & randn_out(Tensor & result, IntList size);
AT_API Tensor & randn_out(Tensor & result, IntList size, Generator * generator);
AT_API Tensor randn_like(const Tensor & self);
AT_API Tensor randn_like(const Tensor & self, const TensorOptions & options);
AT_API Tensor randperm(int64_t n, const TensorOptions & options={});
AT_API Tensor randperm(int64_t n, Generator * generator, const TensorOptions & options={});
AT_API Tensor & randperm_out(Tensor & result, int64_t n);
AT_API Tensor & randperm_out_cpu(Tensor & result, int64_t n, Generator * generator);
AT_API Tensor & randperm_out_cuda(Tensor & result, int64_t n, Generator * generator);
AT_API Tensor range(Scalar start, Scalar end, const TensorOptions & options={});
AT_API Tensor range(Scalar start, Scalar end, Scalar step, const TensorOptions & options={});
AT_API Tensor & range_out(Tensor & result, Scalar start, Scalar end);
AT_API Tensor & range_out(Tensor & result, Scalar start, Scalar end, Scalar step);
AT_API Tensor repeat(const Tensor & self, IntList repeats);
AT_API Tensor reshape(const Tensor & self, IntList shape);
AT_API Tensor reshape_as(const Tensor & self, const Tensor & other);
AT_API std::tuple<Tensor,Tensor> RoiPooling2d_forward_cpu(const Tensor & input, const Tensor & rois, int64_t pooledHeight, int64_t pooledWidth, double spatialScale);
AT_API std::tuple<Tensor,Tensor> RoiPooling2d_forward_cuda(const Tensor & input, const Tensor & rois, int64_t pooledHeight, int64_t pooledWidth, double spatialScale);
AT_API Tensor RoiPooling2d_backward_cpu(const Tensor & input, const Tensor & rois, int64_t pooledHeight, int64_t pooledWidth, double spatialScale, const Tensor & gradOutput, const Tensor & argmaxes);
AT_API Tensor RoiPooling2d_backward_cuda(const Tensor & input, const Tensor & rois, int64_t pooledHeight, int64_t pooledWidth, double spatialScale, const Tensor & gradOutput, const Tensor & argmaxes);
AT_API Tensor round(const Tensor & self);
AT_API Tensor & _round__cpu(Tensor & self);
AT_API Tensor & _round__cuda(Tensor & self);
AT_API Tensor & _round_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _round_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor rrelu(const Tensor & self, Scalar lower=0.125, Scalar upper=0.333333333333, bool training=false, Generator * generator=nullptr);
AT_API Tensor & rrelu_(Tensor & self, Scalar lower=0.125, Scalar upper=0.333333333333, bool training=false, Generator * generator=nullptr);
AT_API Tensor relu(const Tensor & self);
AT_API Tensor & relu_(Tensor & self);
AT_API Tensor hardshrink_cpu(const Tensor & self, Scalar lambd=0.5);
AT_API Tensor hardshrink_cuda(const Tensor & self, Scalar lambd=0.5);
AT_API Tensor hardshrink_backward_cpu(const Tensor & grad_out, const Tensor & self, Scalar lambd);
AT_API Tensor hardshrink_backward_cuda(const Tensor & grad_out, const Tensor & self, Scalar lambd);
AT_API Tensor rsqrt(const Tensor & self);
AT_API Tensor & _rsqrt__cpu(Tensor & self);
AT_API Tensor & _rsqrt__cuda(Tensor & self);
AT_API Tensor & _rsqrt_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _rsqrt_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor select(const Tensor & self, int64_t dim, int64_t index);
AT_API Tensor selu(const Tensor & self);
AT_API Tensor & selu_(Tensor & self);
AT_API Tensor celu(const Tensor & self, Scalar alpha=1.0);
AT_API Tensor & celu_(Tensor & self, Scalar alpha=1.0);
AT_API Tensor sigmoid(const Tensor & self);
AT_API Tensor & _sigmoid__cpu(Tensor & self);
AT_API Tensor & _sigmoid__cuda(Tensor & self);
AT_API Tensor & _sigmoid_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _sigmoid_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor sin(const Tensor & self);
AT_API Tensor & _sin__cpu(Tensor & self);
AT_API Tensor & _sin__cuda(Tensor & self);
AT_API Tensor & _sin_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _sin_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor sinh(const Tensor & self);
AT_API Tensor & _sinh__cpu(Tensor & self);
AT_API Tensor & _sinh__cuda(Tensor & self);
AT_API Tensor & _sinh_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _sinh_out_cuda(Tensor & result, const Tensor & self);
AT_API int64_t size(const Tensor & self, int64_t dim);
AT_API Tensor slice(const Tensor & self, int64_t dim=0, int64_t start=0, int64_t end=9223372036854775807, int64_t step=1);
AT_API std::tuple<Tensor,Tensor> slogdet(const Tensor & self);
AT_API Tensor smm(const Tensor & self, const Tensor & mat2);
AT_API Tensor softmax_cpu(const Tensor & self, int64_t dim);
AT_API Tensor softmax_cuda(const Tensor & self, int64_t dim);
AT_API Tensor softmax_backward_cpu(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self);
AT_API Tensor softmax_backward_cuda(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self);
AT_API Tensor & add_out_sparse_cpu(Tensor & result, const Tensor & self, const Tensor & other, Scalar alpha=1);
AT_API Tensor & add_out_sparse_cuda(Tensor & result, const Tensor & self, const Tensor & other, Scalar alpha=1);
AT_API Tensor & add_out_dense_sparse_cpu(Tensor & result, const Tensor & self, SparseTensorRef other, Scalar alpha=1);
AT_API Tensor & add_out_dense_sparse_cuda(Tensor & result, const Tensor & self, SparseTensorRef other, Scalar alpha=1);
AT_API Tensor & div_out_sparse_zerodim(Tensor & result, const Tensor & self, const Tensor & other);
AT_API Tensor & div_out_sparse_scalar(Tensor & result, const Tensor & self, Scalar other);
AT_API Tensor & mul_out_sparse_cpu(Tensor & result, const Tensor & self, const Tensor & other);
AT_API Tensor & mul_out_sparse_cuda(Tensor & result, const Tensor & self, const Tensor & other);
AT_API Tensor & mul_out_sparse_zerodim(Tensor & result, const Tensor & self, const Tensor & other);
AT_API Tensor & mul_out_sparse_scalar(Tensor & result, const Tensor & self, Scalar other);
AT_API std::vector<Tensor> split(const Tensor & self, int64_t split_size, int64_t dim=0);
AT_API std::vector<Tensor> split_with_sizes(const Tensor & self, IntList split_sizes, int64_t dim=0);
AT_API Tensor squeeze(const Tensor & self);
AT_API Tensor squeeze(const Tensor & self, int64_t dim);
AT_API Tensor & squeeze_(Tensor & self);
AT_API Tensor & squeeze_(Tensor & self, int64_t dim);
AT_API Tensor sspaddmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & _sspaddmm_out_only_sparse(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & _sspaddmm_out_only_sparse_cuda(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & _sspaddmm_out_cpu(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & _sspaddmm_out_cuda(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor stack(TensorList tensors, int64_t dim=0);
AT_API Tensor & stack_out(Tensor & result, TensorList tensors, int64_t dim=0);
AT_API Tensor stft(const Tensor & self, int64_t n_fft, int64_t hop_length, int64_t win_length, const Tensor & window={}, bool normalized=false, bool onesided=true);
AT_API int64_t stride(const Tensor & self, int64_t dim);
AT_API Tensor sum(const Tensor & self, ScalarType dtype);
AT_API Tensor sum(const Tensor & self);
AT_API Tensor _sum_cpu(const Tensor & self);
AT_API Tensor _sum_cuda(const Tensor & self);
AT_API Tensor sum(const Tensor & self, IntList dim, bool keepdim, ScalarType dtype);
AT_API Tensor sum(const Tensor & self, IntList dim, bool keepdim=false);
AT_API Tensor sum(const Tensor & self, IntList dim, ScalarType dtype);
AT_API Tensor _sum(const Tensor & self, IntList dim, bool keepdim=false);
AT_API Tensor & sum_out(Tensor & result, const Tensor & self, IntList dim, bool keepdim, ScalarType dtype);
AT_API Tensor & sum_out(Tensor & result, const Tensor & self, IntList dim, bool keepdim=false);
AT_API Tensor & sum_out(Tensor & result, const Tensor & self, IntList dim, ScalarType dtype);
AT_API Tensor & _sum_out(Tensor & result, const Tensor & self, IntList dim, bool keepdim=false);
AT_API Tensor & _sum_out_cuda(Tensor & result, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor sqrt(const Tensor & self);
AT_API Tensor & _sqrt__cpu(Tensor & self);
AT_API Tensor & _sqrt__cuda(Tensor & self);
AT_API Tensor & _sqrt_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _sqrt_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor std(const Tensor & self, bool unbiased=true);
AT_API Tensor std(const Tensor & self, int64_t dim, bool unbiased=true, bool keepdim=false);
AT_API Tensor & std_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased=true, bool keepdim=false);
AT_API Tensor prod(const Tensor & self, ScalarType dtype);
AT_API Tensor prod(const Tensor & self);
AT_API Tensor _prod_cpu(const Tensor & self);
AT_API Tensor _prod_cuda(const Tensor & self);
AT_API Tensor prod(const Tensor & self, int64_t dim, bool keepdim, ScalarType dtype);
AT_API Tensor prod(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor prod(const Tensor & self, int64_t dim, ScalarType dtype);
AT_API Tensor _prod(const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor & prod_out(Tensor & result, const Tensor & self, int64_t dim, bool keepdim, ScalarType dtype);
AT_API Tensor & prod_out(Tensor & result, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor & prod_out(Tensor & result, const Tensor & self, int64_t dim, ScalarType dtype);
AT_API Tensor & _prod_out_cpu(Tensor & result, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor & _prod_out_cuda(Tensor & result, const Tensor & self, int64_t dim, bool keepdim=false);
AT_API Tensor t(const Tensor & self);
AT_API Tensor & t_(Tensor & self);
AT_API Tensor tan(const Tensor & self);
AT_API Tensor & _tan__cpu(Tensor & self);
AT_API Tensor & _tan__cuda(Tensor & self);
AT_API Tensor & _tan_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _tan_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor tanh(const Tensor & self);
AT_API Tensor & _tanh__cpu(Tensor & self);
AT_API Tensor & _tanh__cuda(Tensor & self);
AT_API Tensor & _tanh_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _tanh_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor transpose(const Tensor & self, int64_t dim0, int64_t dim1);
AT_API Tensor & transpose_(Tensor & self, int64_t dim0, int64_t dim1);
AT_API Tensor flip_cpu(const Tensor & self, IntList dims);
AT_API Tensor flip_cuda(const Tensor & self, IntList dims);
AT_API Tensor rot90(const Tensor & self, int64_t k=1, IntList dims={0,1});
AT_API Tensor _trilinear(const Tensor & i1, const Tensor & i2, const Tensor & i3, IntList expand1, IntList expand2, IntList expand3, IntList sumdim, int64_t unroll_dim=1);
AT_API Tensor triplet_margin_loss(const Tensor & anchor, const Tensor & positive, const Tensor & negative, double margin=1.0, double p=2, double eps=1e-06, bool swap=false, int64_t reduction=Reduction::ElementwiseMean);
AT_API Tensor trunc(const Tensor & self);
AT_API Tensor & _trunc__cpu(Tensor & self);
AT_API Tensor & _trunc__cuda(Tensor & self);
AT_API Tensor & _trunc_out_cpu(Tensor & result, const Tensor & self);
AT_API Tensor & _trunc_out_cuda(Tensor & result, const Tensor & self);
AT_API Tensor type_as(const Tensor & self, const Tensor & other);
AT_API std::tuple<Tensor,Tensor> _unique_cpu(const Tensor & self, bool sorted=false, bool return_inverse=false);
AT_API std::tuple<Tensor,Tensor> _unique_cuda(const Tensor & self, bool sorted=false, bool return_inverse=false);
AT_API Tensor _unsafe_view(const Tensor & self, IntList size);
AT_API Tensor unsqueeze(const Tensor & self, int64_t dim);
AT_API Tensor & unsqueeze_(Tensor & self, int64_t dim);
AT_API Tensor var(const Tensor & self, bool unbiased=true);
AT_API Tensor var(const Tensor & self, int64_t dim, bool unbiased=true, bool keepdim=false);
AT_API Tensor & var_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased=true, bool keepdim=false);
AT_API Tensor view_as(const Tensor & self, const Tensor & other);
AT_API Tensor where(const Tensor & condition, const Tensor & self, const Tensor & other);
AT_API Tensor _s_where_cpu(const Tensor & condition, const Tensor & self, const Tensor & other);
AT_API Tensor _s_where_cuda(const Tensor & condition, const Tensor & self, const Tensor & other);
AT_API Tensor zeros(IntList size, const TensorOptions & options={});
AT_API Tensor & zeros_out(Tensor & result, IntList size);
AT_API Tensor zeros_like(const Tensor & self);
AT_API Tensor zeros_like(const Tensor & self, const TensorOptions & options);
AT_API Tensor _standard_gamma_grad_cpu(const Tensor & self, const Tensor & output);
AT_API Tensor _standard_gamma_grad_cuda(const Tensor & self, const Tensor & output);
AT_API Tensor _s_gamma_cpu(const Tensor & self, Generator * generator=nullptr);
AT_API Tensor _s_gamma_cuda(const Tensor & self, Generator * generator=nullptr);
AT_API Tensor _s_poisson_cpu(const Tensor & self, Generator * generator=nullptr);
AT_API Tensor _s_poisson_cuda(const Tensor & self, Generator * generator=nullptr);
AT_API Tensor norm_sparse(const Tensor & self, Scalar p=2);
AT_API Tensor norm(const Tensor & self, Scalar p=2);
AT_API Tensor norm(const Tensor & self, Scalar p, int64_t dim, bool keepdim=false);
AT_API Tensor & norm_out(Tensor & result, const Tensor & self, Scalar p, int64_t dim, bool keepdim=false);
AT_API Tensor clone_sparse(const Tensor & self);
AT_API Tensor clone(const Tensor & self);
AT_API Tensor & resize_as_sparse_(Tensor & self, const Tensor & the_template);
AT_API Tensor & resize_as_(Tensor & self, const Tensor & the_template);
AT_API Tensor & pow_out_sparse_scalar(Tensor & result, const Tensor & self, Scalar exponent);
AT_API Tensor pow_sparse_scalar(const Tensor & self, Scalar exponent);
AT_API Tensor & pow_out(Tensor & result, const Tensor & self, Scalar exponent);
AT_API Tensor pow(const Tensor & self, Scalar exponent);
AT_API Tensor & zero_sparse_(Tensor & self);
AT_API Tensor & zero_(Tensor & self);
AT_API Tensor & sub_out(Tensor & result, const Tensor & self, const Tensor & other, Scalar alpha=1);
AT_API Tensor sub(const Tensor & self, const Tensor & other, Scalar alpha=1);
AT_API Tensor & sub_(Tensor & self, const Tensor & other, Scalar alpha=1);
AT_API Tensor sub(const Tensor & self, Scalar other, Scalar alpha=1);
AT_API Tensor & sub_(Tensor & self, Scalar other, Scalar alpha=1);
AT_API Tensor & s_addmm_out_sparse_dense_cpu(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & s_addmm_out_sparse_dense_cuda(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor s_addmm_sparse_dense_cpu(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor s_addmm_sparse_dense_cuda(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & s_addmm_sparse_dense_cpu_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & s_addmm_sparse_dense_cuda_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor & addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta=1, Scalar alpha=1);
AT_API Tensor new_sparse(const Type & self_ty);
AT_API Tensor new_with_size_sparse(const Type & self_ty, IntList size);
AT_API Tensor tensor(const Type & dtype);
AT_API Tensor tensor(const Type & dtype, IntList size);
AT_API Tensor new_with_size_sparse(const Type & self_ty, IntList size);
AT_API Tensor new_with_tensor_sparse(const Tensor & indices, const Tensor & values);
AT_API Tensor new_with_tensor_and_size_sparse(const Tensor & indices, const Tensor & values, IntList size);
AT_API Tensor sparse_coo_tensor(const Type & dtype, IntList size);
AT_API Tensor sparse_coo_tensor(const Tensor & indices, const Tensor & values);
AT_API Tensor sparse_coo_tensor(const Tensor & indices, const Tensor & values, IntList size);
AT_API Tensor new_with_tensor_and_size_unsafe_sparse(const Tensor & indices, const Tensor & values, IntList size);
AT_API Tensor _sparse_coo_tensor_unsafe(const Tensor & indices, const Tensor & values, IntList size);
AT_API Tensor & sparse_resize_(Tensor & self, IntList size, int64_t sparseDims, int64_t denseDims);
AT_API Tensor & sparse_resize_and_clear_(Tensor & self, IntList size, int64_t sparseDims, int64_t denseDims);
AT_API Tensor sparse_mask_cpu(const Tensor & self, SparseTensorRef mask);
AT_API Tensor sparse_mask_cuda(const Tensor & self, SparseTensorRef mask);
AT_API Tensor sparse_to_dense(const Tensor & self);
AT_API int64_t _sparseDims_sparse(const Tensor & self);
AT_API int64_t _sparseDims_sparse(const Tensor & self);
AT_API int64_t _denseDims_sparse(const Tensor & self);
AT_API int64_t _denseDims_sparse(const Tensor & self);
AT_API int64_t _nnz_sparse(const Tensor & self);
AT_API Tensor coalesce_sparse_cpu(const Tensor & self);
AT_API Tensor coalesce_sparse_cuda(const Tensor & self);
AT_API bool is_coalesced_sparse(const Tensor & self);
AT_API Tensor _indices_sparse(const Tensor & self);
AT_API Tensor _values_sparse(const Tensor & self);
AT_API Tensor & hspmm_out_sparse_cpu(Tensor & result, const Tensor & mat1, const Tensor & mat2);
AT_API Tensor & hspmm_out_sparse_cuda(Tensor & result, const Tensor & mat1, const Tensor & mat2);
AT_API Tensor hspmm_sparse_cpu(const Tensor & mat1, const Tensor & mat2);
AT_API Tensor hspmm_sparse_cuda(const Tensor & mat1, const Tensor & mat2);
AT_API Tensor & copy_sparse_(Tensor & self, const Tensor & src);
AT_API int64_t numel(const Tensor & self);
AT_API std::vector<Tensor> unbind(const Tensor & self, int64_t dim=0);
AT_API int64_t get_device_sparse_cuda(const Tensor & self);
AT_API int64_t get_device(const Tensor & self);
AT_API std::vector<Tensor> meshgrid(TensorList tensors);
AT_API Scalar _local_scalar(const Tensor & self);
AT_API Scalar _local_scalar_dense_cpu(const Tensor & self);
AT_API Scalar _local_scalar_dense_cuda(const Tensor & self);
AT_API std::tuple<Tensor,Tensor,Tensor> _thnn_fused_lstm_cell_cuda(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias={}, const Tensor & hidden_bias={});
AT_API std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_lstm_cell_backward_cuda(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias);
AT_API std::tuple<Tensor,Tensor> _thnn_fused_gru_cell_cuda(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias={}, const Tensor & hidden_bias={});
AT_API std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_gru_cell_backward_cuda(const Tensor & grad_hy, const Tensor & workspace, bool has_bias);
AT_API std::tuple<Tensor,Tensor,Tensor> lstm(const Tensor & input, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first);
AT_API std::tuple<Tensor,Tensor,Tensor> lstm(const Tensor & data, const Tensor & batch_sizes, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional);
AT_API std::tuple<Tensor,Tensor> gru(const Tensor & input, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first);
AT_API std::tuple<Tensor,Tensor> gru(const Tensor & data, const Tensor & batch_sizes, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional);
AT_API std::tuple<Tensor,Tensor> rnn_tanh(const Tensor & input, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first);
AT_API std::tuple<Tensor,Tensor> rnn_tanh(const Tensor & data, const Tensor & batch_sizes, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional);
AT_API std::tuple<Tensor,Tensor> rnn_relu(const Tensor & input, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first);
AT_API std::tuple<Tensor,Tensor> rnn_relu(const Tensor & data, const Tensor & batch_sizes, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional);
AT_API std::tuple<Tensor,Tensor> lstm_cell(const Tensor & input, TensorList hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih={}, const Tensor & b_hh={});
AT_API Tensor gru_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih={}, const Tensor & b_hh={});
AT_API Tensor rnn_tanh_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih={}, const Tensor & b_hh={});
AT_API Tensor rnn_relu_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih={}, const Tensor & b_hh={});

} // namespace native
} // namespace at
